{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border: none\" align=\"left\">\n",
    "   <tr style=\"border: none\">\n",
    "              <th style=\"border: none\"><font face=\"verdana\" size=\"5\" color=\"black\"><b>IBM Cloud Discovery Lab: Neural Network Model</b></th>\n",
    "      <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=false\" alt=\"Watson Machine Learning icon\" height=\"40\" width=\"40\"></th>\n",
    "   </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Details:\n",
    "\n",
    "This lab was created to demonstrate Watson Studio's capabilities to integrate with open source frameworks and libraries such as Tensorflow and Keras, providing a high level of customization within the Notebooks whether you prefer to program using Python, R or Scala, and using a broad set of tools into IBM Cloud's environment architecture.\n",
    "\n",
    "#### Services and Tools:\n",
    "\n",
    "In addition to Watson Studio, We'll make use of the following IBM Cloud's services:\n",
    "\n",
    "- Apache Spark;\n",
    "- Cloud Object Storage;\n",
    "- Python Web App with Flask;\n",
    "- Watson Machine Learning;\n",
    "\n",
    "\n",
    "#### Datasets used:\n",
    "AIRCRAFT\n",
    "http://image-net.org/synset?wnid=n02686568\n",
    "\n",
    "BIRDS\n",
    "http://image-net.org/synset?wnid=n01503061\n",
    "\n",
    "HUMANS\n",
    "http://image-net.org/synset?wnid=n02472987\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --index-url https://test.pypi.org/simple/ watson-machine-learning-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, urllib3, requests, json, time, requests, wget, base64, glob\n",
    "import shutil, random, tarfile, ibm_boto3, dict, dictionary\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "from skimage import img_as_float\n",
    "from six.moves import urllib\n",
    "from uuid import uuid4\n",
    "from urllib.request import urlopen\n",
    "from botocore.client import Config\n",
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import decode_predictions, preprocess_input\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently, Tensorflow includes a [script](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/image_retraining/retrain.py) that will handle the transfer learning of either [Inception V3 model](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models) or a [MobileNet](https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html). Both were trained on [ImageNet 2012 competition](http://www.image-net.org/challenges/LSVRC/2012/) images (1000 categories and 1.2 million images).\n",
    "\n",
    "**Inception V3**: higher accuracy but slower — Top 1 Accuracy on ImageNet: 78% — ~85MB model size <br />\n",
    "**MobileNets**: smaller and faster, but lower accuracy. — Top 1 Accuracy on ImageNet: 70.7% — ~19MB model size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Inception V3 Retrain architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget -O retrain.py https://raw.githubusercontent.com/tensorflow/tensorflow/7f53659bc67bba5567ea3f0b69710329843e0228/tensorflow/examples/image_retraining/retrain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import architecture labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O label_image.py https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/label_image/label_image.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Training and Testing ImageNet datasets from the dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O trainingset.zip https://www.dropbox.com/s/4unryao4xt8ehgp/training_images.zip?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O testingset.zip https://www.dropbox.com/s/0eu80yday3r4bn8/testing_images.zip?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o ./trainingset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o ./testingset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls  -aLF ./training_images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls  -aLF ./testing_images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('training_images/.DS_Store') \n",
    "os.remove('testing_images/.DS_Store') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = os.listdir('training_images/')\n",
    "images = []\n",
    "for folder in os.listdir('training_images'):\n",
    "    path = os.path.join('training_images', folder)\n",
    "    images.extend([os.path.join(path, f) for f in os.listdir(path)])\n",
    "\n",
    "# Plot some sample images in the dataset.\n",
    "plt.figure(figsize=(20,10))\n",
    "for i in range(15):\n",
    "    img = mpimg.imread(random.choice(images))\n",
    "    plt.subplot(3, 5, i+1)\n",
    "    plt.imshow(img)\n",
    "    frame = plt.gca()\n",
    "    frame.axes.get_xaxis().set_visible(False)\n",
    "    frame.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process to Applying transfer learning using the Inception V3 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally training a model from scratch would take an enormous amount of time and resources. Here, however, we will only be training the final layer of the network, so the training time is much more reasonable.\n",
    "\n",
    "Let's go over some of the arguments we will be using.\n",
    "\n",
    "The ***bottleneck_dir*** will be used to cache the outputs of the lower layers on disk so they don’t have to repeatedly be recalculated. 'Bottleneck' is an informal term often used for the layer just before the final output layer that actually does the classification. Since images are reused several times during training, it would be too time-consuming to calculate the layers before the bottleneck for each image each time we use it. These lower layers never changed, so we can just run the image through them once, then cache and reuse the outputs.\n",
    "\n",
    "\n",
    "The ***how_many_training_steps*** option is used to specify that we want to run this example for 1000 iterations. This amount can be experimented with.\n",
    "\n",
    "The ***model_dir*** option asks us where to store the trained model.\n",
    "\n",
    "The ***summaries_dir*** option asks us where to save summary logs for TensorBoard (which we won't be using here).\n",
    "\n",
    "The ***output_graph*** option is where the script will write out a version of the Inception v3 neural network with a final layer retrained to our categories. \n",
    "\n",
    "The ***output_labels*** will be the file where the labels are stored. These labels are the same as the image folder names.\n",
    "\n",
    "Lastly, we use the ***image_dir*** argument to pass in the directory containing the labeled class folders containing our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python retrain.py \\\n",
    "    --bottleneck_dir=./ml-model/bottlenecks \\\n",
    "    --how_many_training_steps 1000 \\\n",
    "    --learning_rate 0.01 \\\n",
    "    --train_batch_size 200 \\\n",
    "    --model_dir=./ml-model/pretrained_model \\\n",
    "    --summaries_dir=./retrain-logs \\\n",
    "    --output_graph=./ml-model/retrained_graph.pb \\\n",
    "    --output_labels=./ml-model/retrained_labels.txt \\\n",
    "    --image_dir=./training_images/ \\\n",
    "    --saved_model_dir =./saved-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Concepts\n",
    "\n",
    "As the retraining script runs, you'll see a series of step outputs, each showing the following information:\n",
    "\n",
    "* The **training accuracy** shows the percentage of the images used in the current training batch that were correctly labeled\n",
    "* **Validation accuracy**: The validation accuracy is the precision (percentage of correctly-labelled images) on a randomly-selected group of images from a different set.\n",
    "* **Cross entropy** is a loss function that shows how well the learning process is progressing (the lower the better).\n",
    "\n",
    "If the *training accuracy* is high but the *validation accuracy* stays low, the model is overfitting or memorizing specific features in the training images that don't help it classify images more generally.\n",
    "\n",
    "When training keep an eye on the *cross entropy*. The goal is to get this value as small as possible, and you can tell if the model is learning by if the loss is trending downwards or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This visualization code taken from: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = tf.compat.as_bytes(\"<stripped %d bytes>\"%size)\n",
    "    return strip_def\n",
    "  \n",
    "def rename_nodes(graph_def, rename_func):\n",
    "    res_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = res_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        n.name = rename_func(n.name)\n",
    "        for i, s in enumerate(n.input):\n",
    "            n.input[i] = rename_func(s) if s[0]!='^' else '^'+rename_func(s[1:])\n",
    "    return res_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "  \n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.gfile.FastGFile(\"./ml-model/retrained_graph.pb\", 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "\n",
    "    # Parse the graph.\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    show_graph(graph_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the testing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories_testing = os.listdir('testing_images/')\n",
    "images_testing = []\n",
    "for folder in os.listdir('testing_images'):\n",
    "    path_testing = os.path.join('testing_images', folder)\n",
    "    images.extend([os.path.join(path_testing, f) for f in os.listdir(path_testing)])\n",
    "\n",
    "# Plot some sample images in the dataset.\n",
    "plt.figure(figsize=(20,10))\n",
    "for i in range(15):\n",
    "    img = mpimg.imread(random.choice(images))\n",
    "    plt.subplot(3, 5, i+1)\n",
    "    plt.imshow(img)\n",
    "    frame = plt.gca()\n",
    "    frame.axes.get_xaxis().set_visible(False)\n",
    "    frame.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the model and providing an image path for a test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_dir = './ml-model'\n",
    "\n",
    "test_image = './testing_images/validation_aircrafts/n04583620_4216.JPEG'\n",
    "\n",
    "input_layer = 'Mul'\n",
    "input_height = 299\n",
    "input_width = 299\n",
    "\n",
    "\n",
    "%env MODEL_DIR=$model_dir\n",
    "%env INPUT_HEIGHT=$input_height\n",
    "%env INPUT_WIDTH=$input_width\n",
    "%env TEST_IMAGE=$test_image\n",
    "%env INPUT_LAYER=$input_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the scripts and getting the results of the Image Recognition tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = mpimg.imread(test_image)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(img)\n",
    "frame = plt.gca()\n",
    "frame.axes.get_xaxis().set_visible(False)\n",
    "frame.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "!python ./label_image.py \\\n",
    "    --graph=$MODEL_DIR/retrained_graph.pb --labels=$MODEL_DIR/retrained_labels.txt \\\n",
    "    --input_layer=$INPUT_LAYER \\\n",
    "    --output_layer=final_result \\\n",
    "    --input_height=$INPUT_HEIGHT --input_width=$INPUT_WIDTH \\\n",
    "    --image=$TEST_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Creating REST API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_credentials = {\n",
    "  \"insert your credentials here\"\n",
    "}\n",
    "\n",
    "auth_endpoint = 'https://iam.bluemix.net/oidc/token'\n",
    "service_endpoint = 'https://s3-api.us-geo.objectstorage.softlayer.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = ibm_boto3.resource('s3',\n",
    "                         ibm_api_key_id=cos_credentials['apikey'],\n",
    "                         ibm_service_instance_id=cos_credentials['resource_instance_id'],\n",
    "                         ibm_auth_endpoint=auth_endpoint,\n",
    "                         config=Config(signature_version='oauth'),\n",
    "                         endpoint_url=service_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "bucket_uid = str(uuid4())\n",
    "buckets = ['training-data-' + bucket_uid, 'training-results-' + bucket_uid]\n",
    "\n",
    "for bucket in buckets:\n",
    "    if not cos.Bucket(bucket) in cos.buckets.all():\n",
    "        print('Creating bucket \"{}\"...'.format(bucket))\n",
    "        try:\n",
    "            cos.create_bucket(Bucket=bucket)\n",
    "        except ibm_boto3.exceptions.ibm_botocore.client.ClientError as e:\n",
    "            print('Error: {}.'.format(e.response['Error']['Message']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_links = ['http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "              'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "              'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "              'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "bucket_obj = cos.Bucket(buckets[0])\n",
    "\n",
    "for data_link in data_links:\n",
    "    filename=data_link.split('/')[-1]\n",
    "    print('Uploading data {}...'.format(filename))\n",
    "    with urlopen(data_link) as data:\n",
    "        bucket_obj.upload_fileobj(data, filename)\n",
    "        print('{} is uploaded.'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bucket_name in buckets:\n",
    "    print(bucket_name)\n",
    "    bucket_obj = cos.Bucket(bucket_name)\n",
    "    for obj in bucket_obj.objects.all():\n",
    "        print(\"  File: {}, {:4.2f}kB\".format(obj.key, obj.size/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Watson Machine Learning client API services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_credentials = {\n",
    "  \"Insert your credentials here\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $PIP_BUILD/watson-machine-learning-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install watson-machine-learning-client --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = WatsonMachineLearningAPIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir_path = '=/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties to integrate the model with the Watson Machine Learning (WML) API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_meta_props = {client.repository.ModelMetaNames.NAME: \"ibm_disco_lab\",\n",
    "                            client.repository.ModelMetaNames.AUTHOR_NAME: \"Jorge Chagas\",\n",
    "                            client.repository.ModelMetaNames.AUTHOR_EMAIL: \"jorge.barbosa@ibm.com\",\n",
    "                            client.repository.ModelMetaNames.FRAMEWORK_NAME: \"tensorflow\",\n",
    "                            client.repository.ModelMetaNames.FRAMEWORK_VERSION: \"1.5\",\n",
    "                            client.repository.ModelMetaNames.RUNTIME_NAME: \"python\",\n",
    "                            client.repository.ModelMetaNames.RUNTIME_VERSION: \"3.5\"}\n",
    "published_model_details = client.repository.store_model(model=model_dir_path, meta_props=model_meta_props, training_data='./training_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_uid = client.repository.get_model_uid(published_model_details) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_details = client.repository.get_details(definition_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model details into the WML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(model_details, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.repository.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = client.repository.load(definition_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Url: \" + client.repository.get_model_url(model_details))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uid = client.repository.get_model_uid(model_details)\n",
    "print(\"Saved model uid: \" + model_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification for the model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_details = client.deployments.create(model_uid, \"IBM Disco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_url =  client.deployments.get_scoring_url(deployment_details)\n",
    "print(scoring_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://us-south.ml.cloud.ibm.com'\n",
    "username = 'your_wml_username'\n",
    "password = 'your_wml_password\n",
    "scoring_endpoint = scoring_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = urllib3.util.make_headers(basic_auth='{}:{}'.format(username, password))\n",
    "path = '{}/v3/identity/token'.format(url)\n",
    "response = requests.get(path, headers=headers)\n",
    "mltoken = json.loads(response.text).get('token')\n",
    "print(mltoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img('./testing_images/validation_aircrafts/n04583620_4216.JPEG',target_size=(299,299))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = image.img_to_array(img)\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "input_image = preprocess_input(input_image).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_data = {'values': input_image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = client.deployments.score(scoring_url, scoring_data)\n",
    "print(\"Scoring result: \" + str(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.get('values')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
